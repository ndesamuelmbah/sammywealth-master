{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Motivation:</h1> \n",
    "<p>This jupyter notebook is meant to serve as a complementary material for my tutorial on How to use Amazon Comprehend.</p>\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <h3> This notebook is designed to showcase how to use AMAZON COMPREHEND for three tasks.</h3>\n",
    "    <li> Sentiment Analysis.</li>\n",
    "    <li> Key Phrase Extraction. </li>\n",
    "    <li> Named Entities Extraction. </li>\n",
    "    <li> Language Detection</li>\n",
    "</ul>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Boto3 and connect its client to the comprehend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "comprehend = boto3.client(service_name='comprehend', region_name='us-east-1',aws_access_key_id = 'Your Access Key Id Goes Here', \n",
    "                          aws_secret_access_key = 'You Secret Key Goes Here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are all set to use Comprehend! \n",
    "\n",
    "\n",
    "<h1> 1) Comprehend for Sentiment Analysis.</h1>\n",
    "<p> It is simple. Just use the __detect_sentiment__ method of your client.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sentiment': 'POSITIVE',\n",
       " 'SentimentScore': {'Positive': 0.9986692667007446,\n",
       "  'Negative': 5.039777897763997e-05,\n",
       "  'Neutral': 0.00023834838066250086,\n",
       "  'Mixed': 0.0010420106118544936},\n",
       " 'ResponseMetadata': {'RequestId': '83970793-e89e-11e8-97c2-ddc68243f710',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Thu, 15 Nov 2018 06:20:14 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '167',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '83970793-e89e-11e8-97c2-ddc68243f710'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = comprehend.detect_sentiment(Text = 'Amazon is a great company. I love working there.', LanguageCode= 'en')\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The output is in java script object notation jason format. In python, this is equivallent to a nested dictionary. The output consists of three main parts namely:</p> \n",
    "<ul>\n",
    "    <li>__Sentiment__: This is a word that describes the sentiment expressed in the Text.</li>\n",
    "    <li>__SentimentScore__: This is a dictionary of sentiments wiith their probabilities of represented in the Text.</li>\n",
    "    <li>__ResponseMetadata__: This is the metadata about the call to the comphrehend service.</li>\n",
    "</ul>\n",
    "<p> They are four valid sentiments namely: </p>\n",
    "## POSITIVE | NEGATIVE | NEUTRAL | MIXED\n",
    "\n",
    "<br> You can only have one sentiment per Text</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 2) Comprehend for Key Phrase Extraction.</h1>\n",
    "<p> It is simple. Just use the __detect_key_phrases__ method of your client.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the text that you want to get key phrases from\n",
    "txt = \"On the 10th of November 2016, at 6:30 pm, the Police arrested John Doe, the director of Free Health at his home in Madison. For several months now, he has been under investigation. John was accused of diverting company funds into his personal account. He has requested to speak with his lawyer as soon as possible.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keyPhrases = comprehend.detect_key_phrases(Text = txt, LanguageCode = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KeyPhrases': [{'Score': 0.9808025360107422,\n",
       "   'Text': 'the 10th',\n",
       "   'BeginOffset': 3,\n",
       "   'EndOffset': 11},\n",
       "  {'Score': 0.9979576468467712,\n",
       "   'Text': 'November 2016',\n",
       "   'BeginOffset': 15,\n",
       "   'EndOffset': 28},\n",
       "  {'Score': 0.9829525351524353,\n",
       "   'Text': '6:30 pm',\n",
       "   'BeginOffset': 33,\n",
       "   'EndOffset': 40},\n",
       "  {'Score': 0.9958406090736389,\n",
       "   'Text': 'the Police',\n",
       "   'BeginOffset': 42,\n",
       "   'EndOffset': 52},\n",
       "  {'Score': 0.9991291761398315,\n",
       "   'Text': 'John Doe',\n",
       "   'BeginOffset': 62,\n",
       "   'EndOffset': 70},\n",
       "  {'Score': 0.999496579170227,\n",
       "   'Text': 'the director',\n",
       "   'BeginOffset': 72,\n",
       "   'EndOffset': 84},\n",
       "  {'Score': 0.9977135062217712,\n",
       "   'Text': 'Free Health',\n",
       "   'BeginOffset': 88,\n",
       "   'EndOffset': 99},\n",
       "  {'Score': 0.9993090629577637,\n",
       "   'Text': 'his home',\n",
       "   'BeginOffset': 103,\n",
       "   'EndOffset': 111},\n",
       "  {'Score': 0.9967743754386902,\n",
       "   'Text': 'Madison',\n",
       "   'BeginOffset': 115,\n",
       "   'EndOffset': 122},\n",
       "  {'Score': 0.9980320930480957,\n",
       "   'Text': 'several months',\n",
       "   'BeginOffset': 128,\n",
       "   'EndOffset': 142},\n",
       "  {'Score': 0.9969323873519897,\n",
       "   'Text': 'investigation',\n",
       "   'BeginOffset': 166,\n",
       "   'EndOffset': 179},\n",
       "  {'Score': 0.9997968077659607,\n",
       "   'Text': 'John',\n",
       "   'BeginOffset': 181,\n",
       "   'EndOffset': 185},\n",
       "  {'Score': 0.9966952800750732,\n",
       "   'Text': 'company\\xa0funds',\n",
       "   'BeginOffset': 211,\n",
       "   'EndOffset': 224},\n",
       "  {'Score': 0.9996809959411621,\n",
       "   'Text': 'his personal account',\n",
       "   'BeginOffset': 230,\n",
       "   'EndOffset': 250},\n",
       "  {'Score': 0.9976174235343933,\n",
       "   'Text': 'his lawyer',\n",
       "   'BeginOffset': 283,\n",
       "   'EndOffset': 293}],\n",
       " 'ResponseMetadata': {'RequestId': 'a2876925-e89e-11e8-90dd-75ee00afd85e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Thu, 15 Nov 2018 06:21:06 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1254',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'a2876925-e89e-11e8-90dd-75ee00afd85e'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyPhrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The key phrases are in jave script object notation jason which in python is the equivallent of nested dictionaries. The output consists of KeyPhrases, and ResponseMetadata. We are only interested in the KeyPhrases. Each KeyPhrase is dictionary of four objects namely:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>__Score__: The Score is measure of the confidence the algorithm has that that key phrase makes sense.</li>\n",
    "    <li>__Text__: This is our subject of interest. This is the text that make up the key phrase that we want.</li>\n",
    "    <li>__BeginOffset__: This is the index of the first character of the key phrase in the input Text.</li>\n",
    "    <li>__EndOffset__: This is the index of the Last character of the key phrase in the input Text.</li>\n",
    "<ul>\n",
    "    \n",
    "    \n",
    "<br> Below, I print the extracted key phrases, one on a line.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 10th\n",
      "November 2016\n",
      "6:30 pm\n",
      "the Police\n",
      "John Doe\n",
      "the director\n",
      "Free Health\n",
      "his home\n",
      "Madison\n",
      "several months\n",
      "investigation\n",
      "John\n",
      "company funds\n",
      "his personal account\n",
      "his lawyer\n"
     ]
    }
   ],
   "source": [
    "for result in keyPhrases['KeyPhrases']:\n",
    "    print(result['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 2) Comprehend for Named Entity Extraction.</h1>\n",
    "<p>It is simple. Just use the detect_named_entities method of your client.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedEntities = comprehend.detect_entities(Text = txt, LanguageCode = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Entities': [{'Score': 0.984207272529602,\n",
       "   'Type': 'DATE',\n",
       "   'Text': '10th of November 2016',\n",
       "   'BeginOffset': 7,\n",
       "   'EndOffset': 28},\n",
       "  {'Score': 0.9976122975349426,\n",
       "   'Type': 'DATE',\n",
       "   'Text': '6:30 pm',\n",
       "   'BeginOffset': 33,\n",
       "   'EndOffset': 40},\n",
       "  {'Score': 0.9987821578979492,\n",
       "   'Type': 'PERSON',\n",
       "   'Text': 'John Doe',\n",
       "   'BeginOffset': 62,\n",
       "   'EndOffset': 70},\n",
       "  {'Score': 0.7706741094589233,\n",
       "   'Type': 'ORGANIZATION',\n",
       "   'Text': 'Free Health',\n",
       "   'BeginOffset': 88,\n",
       "   'EndOffset': 99},\n",
       "  {'Score': 0.9977292418479919,\n",
       "   'Type': 'LOCATION',\n",
       "   'Text': 'Madison',\n",
       "   'BeginOffset': 115,\n",
       "   'EndOffset': 122},\n",
       "  {'Score': 0.7375115752220154,\n",
       "   'Type': 'QUANTITY',\n",
       "   'Text': 'several months',\n",
       "   'BeginOffset': 128,\n",
       "   'EndOffset': 142},\n",
       "  {'Score': 0.9996035695075989,\n",
       "   'Type': 'PERSON',\n",
       "   'Text': 'John',\n",
       "   'BeginOffset': 181,\n",
       "   'EndOffset': 185}],\n",
       " 'ResponseMetadata': {'RequestId': 'fcc4feb2-e8a0-11e8-b6e5-5dbbcf180c8c',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Thu, 15 Nov 2018 06:37:56 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '705',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'fcc4feb2-e8a0-11e8-b6e5-5dbbcf180c8c'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namedEntities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Just like in Key phrase extraction, the output is a json object. Each named entity consists of 5 parts namely:</p>\n",
    "<ul>\n",
    "    <li>__Score__: The Score is measure of the confidence the algorithm has that that key phrase makes sense.</li>\n",
    "    <li>__Type__: This is the type of named entity that our named entity is. They are 9 valid types.</li>\n",
    "    <li>__Text__: This is our subject of interest. This is the text that make up the key phrase that we want.</li>\n",
    "    <li>__BeginOffset__: This is the index of the first character of the key phrase in the input Text.</li>\n",
    "    <li>__EndOffset__: This is the index of the Last character of the key phrase in the input Text.</li>\n",
    "</ul>\n",
    "    \n",
    "<p>The valid named entity types are:</p>\n",
    "###  PERSON | LOCATION | ORGANIZATION | COMMERCIAL_ITEM | EVENT | DATE | QUANTITY | TITLE | OTHER\n",
    "<br> Below, I print the extracted named entities, and their types one on a line.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: 10th of November 2016 \t EntityType: DATE\n",
      "Entity: 6:30 pm \t EntityType: DATE\n",
      "Entity: John Doe \t EntityType: PERSON\n",
      "Entity: Free Health \t EntityType: ORGANIZATION\n",
      "Entity: Madison \t EntityType: LOCATION\n",
      "Entity: several months \t EntityType: QUANTITY\n",
      "Entity: John \t EntityType: PERSON\n"
     ]
    }
   ],
   "source": [
    "for result in namedEntities['Entities']:\n",
    "    print('Entity: {} \\t EntityType: {}'.format(result['Text'], result['Type']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 3) Comprehend for Language Detection.</h1>\n",
    "<p>It is simple. Just use the __detect_dominant_language__ method of your client.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = comprehend.detect_dominant_language(Text = txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Languages': [{'LanguageCode': 'en', 'Score': 0.9971848130226135}],\n",
       " 'ResponseMetadata': {'RequestId': 'f7b33f6a-e8a4-11e8-90dd-75ee00afd85e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Thu, 15 Nov 2018 07:06:26 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '64',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'f7b33f6a-e8a4-11e8-90dd-75ee00afd85e'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang ### Can you interprete the output in the cell above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h1> 4) Comprehend for Medical Entity detection.</h1>\n",
    "<p>It is simple. Just use the __detect_named_entiti__ method of your client.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h1> RESOURCES </h1>\n",
    "<ul>\n",
    "    <li>__Comprehed [Sentiment Analysis](https://docs.aws.amazon.com/comprehend/latest/dg/how-sentiment.html) How to Page:__</li>\n",
    "    <li>__Comprehed [Key Phrase Extraction](https://docs.aws.amazon.com/comprehend/latest/dg/get-started-api-key-phrases.html) How to Page:__</li>\n",
    "    <li>__Comprehed [Named Entities Extraction](https://docs.aws.amazon.com/comprehend/latest/dg/API_Entity.html) How to Page:__</li>\n",
    "    <li>__Comprehed [Dominant Language Detection](https://docs.aws.amazon.com/comprehend/latest/dg/how-languages.html) How to Page:__</li>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
